{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An MNIST classification using skeletal graphs. \n",
    "\n",
    "A demo on using SN-graphs as inputs to Graph Neural Networks. The model achieves roughly 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST and show some sn-graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "import sn_graph as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# Reshape to (n_samples, 28, 28)\n",
    "X = X.reshape(-1, 28, 28)\n",
    "\n",
    "# Normalize\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 20\n",
    "graphs = []\n",
    "images = []\n",
    "\n",
    "for img in X[:NUM_IMAGES]:\n",
    "    images.append(img)\n",
    "    graphs.append(\n",
    "        sn.create_sn_graph(\n",
    "            img,\n",
    "            minimal_sphere_radius=1,\n",
    "            edge_sphere_threshold=0.9,\n",
    "            edge_threshold=0.9,\n",
    "            return_sdf=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(graphs)):\n",
    "    vertices, edges, sdf_array = graphs[i]\n",
    "    img = sn.draw_sn_graph(vertices, edges, sdf_array, background_image=images[i])\n",
    "\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(\n",
    "        f\"\"\"Label: {y[i]}\\n  Number of vertices: {len(vertices)} \\n Number of edges: {len(edges)}\"\"\"\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dataset, Graph Neural Network, and training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class Mnist_Graph_Dataset(Dataset):\n",
    "    def __init__(self, precompute_graphs=False):\n",
    "        print(\"Initialising dataset...\")\n",
    "        X, targets = self._load_mnist()\n",
    "        self.X = X\n",
    "        self.targets = targets\n",
    "        if precompute_graphs:\n",
    "            self.graphs = [\n",
    "                self._getitem(index)\n",
    "                for index in tqdm(range(len(self.X)), \"Precomputing graphs\")\n",
    "            ]\n",
    "        else:\n",
    "            self.graphs = None\n",
    "\n",
    "    def _load_mnist(self):\n",
    "        # Load MNIST\n",
    "        X, targets = fetch_openml(\n",
    "            \"mnist_784\", version=1, return_X_y=True, as_frame=False\n",
    "        )\n",
    "\n",
    "        # Reshape to (n_samples, 28, 28)\n",
    "        X = X.reshape(-1, 28, 28)\n",
    "\n",
    "        # Normalize\n",
    "        X = X / 255.0\n",
    "\n",
    "        return X, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def _graph_to_tensor(self, vertex_coords, edges, sdf_values, use_pos_features=True):\n",
    "        vertex_coords = np.array(vertex_coords)\n",
    "        pos = torch.tensor(vertex_coords, dtype=torch.float)\n",
    "\n",
    "        coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(vertex_coords)}\n",
    "\n",
    "        src_indices = []\n",
    "        dst_indices = []\n",
    "        edge_lengths = []\n",
    "\n",
    "        for start_coord, end_coord in edges:\n",
    "            src_indices.append(coord_to_idx[tuple(start_coord)])\n",
    "            dst_indices.append(coord_to_idx[tuple(end_coord)])\n",
    "            edge_lengths.append(\n",
    "                np.linalg.norm(np.array(end_coord) - np.array(start_coord))\n",
    "            )\n",
    "\n",
    "        edge_index = torch.tensor([src_indices, dst_indices], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_lengths, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        x = torch.tensor(\n",
    "            [sdf_values[tuple(coord)] for coord in vertex_coords], dtype=torch.float\n",
    "        ).view(-1, 1)\n",
    "        pos_enc = pos / pos.max(dim=0)[0]\n",
    "\n",
    "        if use_pos_features:\n",
    "            # Stack SDF values with positional features\n",
    "            x = torch.cat([x, pos_enc], dim=1)\n",
    "            geom_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        else:\n",
    "            geom_data = Data(\n",
    "                x=x, edge_index=edge_index, edge_attr=edge_attr, pos=pos_enc\n",
    "            )\n",
    "\n",
    "        return geom_data\n",
    "\n",
    "    def _getitem(self, index):\n",
    "        img = X[index]\n",
    "\n",
    "        graph = sn.create_sn_graph(\n",
    "            img,\n",
    "            minimal_sphere_radius=1,\n",
    "            edge_sphere_threshold=0.8,\n",
    "            edge_threshold=0.9,\n",
    "            return_sdf=True,\n",
    "        )\n",
    "\n",
    "        if len(graph[0]) < 2 or len(graph[1]) < 2:\n",
    "            return self._getitem(index + 1)\n",
    "        geom_data = self._graph_to_tensor(*graph)\n",
    "        geom_data.y = torch.tensor([int(self.targets[index])], dtype=torch.long)\n",
    "        return geom_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.graphs[index] if self.graphs is not None else self._getitem(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Mnist_Graph_Dataset(precompute_graphs=True)\n",
    "\n",
    "print(f\"Length of the dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_max_pool\n",
    "\n",
    "\n",
    "class GraphClassificationNetwork(torch.nn.Module):\n",
    "    def __init__(self, vertex_features_dim, edge_features_dim, no_classes):\n",
    "        super().__init__()\n",
    "        # if binary segmentation, then choose one class as number of classes.\n",
    "        self.no_classes = no_classes\n",
    "\n",
    "        self.conv1 = GATConv(\n",
    "            vertex_features_dim, 8, heads=2, edge_dim=edge_features_dim\n",
    "        )\n",
    "        self.conv2 = GATConv(16, 32, heads=2, edge_dim=2)\n",
    "        self.conv3 = GATConv(64, 128, edge_dim=2)\n",
    "        self.lin1 = nn.Linear(128, 32)\n",
    "        self.lin2 = nn.Linear(32, no_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Graph convolutions\n",
    "\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x, (edge_index, edge_attr) = self.conv1(\n",
    "            x, edge_index, edge_attr, return_attention_weights=True\n",
    "        )\n",
    "        x = F.relu(x)\n",
    "        x, (edge_index, edge_attr) = self.conv2(\n",
    "            x, edge_index, edge_attr, return_attention_weights=True\n",
    "        )\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global max pooling - will automatically handle both batched and single samples\n",
    "        batch = getattr(data, \"batch\", None)\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        x = global_max_pool(x, batch)\n",
    "\n",
    "        # FC layers for graph-level prediction\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphClassificationNetwork(\n",
    "    vertex_features_dim=3, edge_features_dim=1, no_classes=10\n",
    ")\n",
    "\n",
    "output = model(dataset[0])\n",
    "\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Assuming dataset and model are defined\n",
    "indices = list(range(len(dataset)))\n",
    "print(\"Creating train-test split\")\n",
    "\n",
    "# Train-test split\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# Create datasets using torch.utils.data.Subset\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "print(\"Creating train loader\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "print(\"Creating test loader\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphClassificationNetwork(\n",
    "    vertex_features_dim=3, edge_features_dim=1, no_classes=10\n",
    ").to(device)\n",
    "# 0.001 or 0.0005 was the best so far!\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Change to BCELoss for binary classification\n",
    "\n",
    "\n",
    "def train(loader, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in tqdm(loader, desc=f\"Epoch {epoch}\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.num_graphs\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def test(loader, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc=f\"Epoch {epoch}\"):\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Forward pass only\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == data.y).sum().item()\n",
    "            total += data.num_graphs\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train(train_loader, epoch)\n",
    "    test_loss, test_acc = test(test_loader, epoch)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "print(f\"Best test accuracy: {best_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sn-graph-5qBlOgtN-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
